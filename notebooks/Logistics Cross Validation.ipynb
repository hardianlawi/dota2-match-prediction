{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     6,
     29,
     41,
     47
    ]
   },
   "outputs": [],
   "source": [
    "radiant_cols = ['hero_' + str(i) for i in range(5)]\n",
    "dire_cols = ['hero_' + str(i) for i in range(5, 10)]\n",
    "no_heroes = 116\n",
    "\n",
    "filter_cols = ['base_mr']\n",
    "\n",
    "cont_variables = [\n",
    "    'agi_gain',\n",
    "    'attack_range',\n",
    "    'attack_rate',\n",
    "    'base_agi',\n",
    "    'base_armor',\n",
    "    'base_attack_max',\n",
    "    'base_attack_min',\n",
    "    'base_health_regen',\n",
    "    'base_int',\n",
    "    'base_mr',\n",
    "    'base_str',\n",
    "    'int_gain',\n",
    "    'legs',\n",
    "    'move_speed',\n",
    "    'pro_ban',\n",
    "    'pro_pick',\n",
    "    'pro_win',\n",
    "    'projectile_speed',\n",
    "    'str_gain',\n",
    "    'turn_rate',\n",
    "]\n",
    "\n",
    "unique_roles = [\n",
    "    'Carry',\n",
    "    'Disabler',\n",
    "    'Durable',\n",
    "    'Escape',\n",
    "    'Initiator',\n",
    "    'Jungler',\n",
    "    'Nuker',\n",
    "    'Pusher',\n",
    "    'Support'\n",
    "]\n",
    "\n",
    "unique_primary_attrs = [\n",
    "    'agi',\n",
    "    'int',\n",
    "    'str'\n",
    "]\n",
    "\n",
    "feature_names = cont_variables + \\\n",
    "    ['min_' + col for col in cont_variables if col not in filter_cols] + \\\n",
    "    ['max_' + col for col in cont_variables if col not in filter_cols] +  [\n",
    "    'no_agi',\n",
    "    'no_int',\n",
    "    'no_str',\n",
    "    'no_melees',\n",
    "    'no_Carry',\n",
    "    'no_Disabler',\n",
    "    'no_Durable',\n",
    "    'no_Escape',\n",
    "    'no_Initiator',\n",
    "    'no_Jungler',\n",
    "    'no_Nuker',\n",
    "    'no_Pusher',\n",
    "    'no_Support',\n",
    "]\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    df = pd.read_csv('../data/matches_data.csv')\n",
    "    radiants = pd.read_csv('../data/radiant_features.csv')\n",
    "    dires = pd.read_csv('../data/dire_features.csv')\n",
    "    \n",
    "    assert np.all(radiants.columns == dires.columns), 'Radiants have different features than dires'\n",
    "    assert df.shape[0] == radiants.shape[0], 'Number of matches in radiants are different to original matches data'\n",
    "    assert df.shape[0] == dires.shape[0], 'Number of matches in dires are different to original matches data'\n",
    "    assert len(feature_names) + 1 == radiants.shape[1]\n",
    "    assert len(feature_names) + 1 == dires.shape[1]\n",
    "    assert (radiants['match_id'] == dires['match_id']).all()\n",
    "    assert (radiants['match_id'] == df['match_id']).all()\n",
    "    \n",
    "    radiants = radiants.\\\n",
    "        rename({col: 'radiant_' + col for col in feature_names}, axis=1).\\\n",
    "        drop('match_id', axis=1)\n",
    "    dires = dires.\\\n",
    "        rename({col: 'dire_' + col for col in feature_names}, axis=1).\\\n",
    "        drop('match_id', axis=1)\n",
    "        \n",
    "    df = pd.concat([df[['match_id', 'radiant_win']], radiants, dires], axis=1)\n",
    "    \n",
    "    return radiants, dires, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiants, dires, df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "0.001  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01  0.5  1  1.5  2  3  7  10  15  20  25  \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits, shuffle=True, random_state=10)\n",
    "\n",
    "print('=================================================')\n",
    "\n",
    "avg_score = {}\n",
    "scores = {'model': [], 'model_1': []}\n",
    "losses = {'model': [], 'model_1': []}\n",
    "\n",
    "l_values = [0.001, 0.01, 0.5, 1, 1.5, 2, 3, 7, 10, 15, 20, 25]\n",
    "\n",
    "for l in l_values:\n",
    "    \n",
    "    print(l, ' ', end='')\n",
    "    \n",
    "    C = 1 / l\n",
    "    \n",
    "    score = dict(model=0, model_1=0)\n",
    "    loss = dict(model=0, model_1=0)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
    "\n",
    "        model = LogisticRegression(random_state=0, solver='saga', C=C, penalty='l2', n_jobs=4, max_iter=200)\n",
    "        model_1 = LogisticRegression(random_state=0, solver='saga', C=C, penalty='l1', n_jobs=4, max_iter=200)\n",
    "        \n",
    "        X_train = np.concatenate([radiants.iloc[train_idx].copy(), dires.iloc[train_idx].copy()], axis=1)\n",
    "        X_test = np.concatenate([radiants.iloc[test_idx].copy(), dires.iloc[test_idx].copy()], axis=1)\n",
    "\n",
    "        y_train = df.radiant_win.iloc[train_idx].values\n",
    "        y_test = df.radiant_win.iloc[test_idx].values\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        model_1.fit(X_train, y_train)\n",
    "        \n",
    "        ypreds = model.predict_proba(X_test)[:, 1]\n",
    "        ypreds_1 = model_1.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        score['model'] += accuracy_score(y_test, ypreds > 0.5)\n",
    "        loss['model'] += log_loss(y_test, ypreds)\n",
    "        \n",
    "        score['model_1'] += accuracy_score(y_test, ypreds_1 > 0.5)\n",
    "        loss['model_1'] += log_loss(y_test, ypreds_1)\n",
    "        \n",
    "        \n",
    "    for k in ['model', 'model_1']:\n",
    "        \n",
    "        score[k] /= n_splits\n",
    "        scores[k].append(score)\n",
    "\n",
    "        loss[k] /= n_splits\n",
    "        losses[k].append(loss)\n",
    "    \n",
    "print('\\nDone.')\n",
    "          \n",
    "############################################################################\n",
    "\n",
    "# print('Avg threshold', avg_threshold / (n_splits * len(models_dict)), '\\n\\n')\n",
    "# print('Number of features:', len(feature_columns))\n",
    "# print('Features:', feature_columns, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
