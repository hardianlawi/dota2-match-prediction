{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.utils.models import define_model, train_and_pred\n",
    "from src.utils.data import load_data\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     6,
     29,
     41
    ]
   },
   "outputs": [],
   "source": [
    "radiant_cols = ['hero_' + str(i) for i in range(5)]\n",
    "dire_cols = ['hero_' + str(i) for i in range(5, 10)]\n",
    "no_heroes = 116\n",
    "\n",
    "radiants, dires, df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "0.001  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardian_lawi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01  0.5  1  1.5  2  3  7  10  15  20  25  \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits, shuffle=True, random_state=10)\n",
    "\n",
    "print('=================================================')\n",
    "\n",
    "avg_score = {}\n",
    "scores = {'model': [], 'model_1': []}\n",
    "losses = {'model': [], 'model_1': []}\n",
    "\n",
    "l_values = [0.001, 0.01, 0.5, 1, 1.5, 2, 3, 7, 10, 15, 20, 25]\n",
    "\n",
    "for l in l_values:\n",
    "    \n",
    "    print(l, ' ', end='')\n",
    "    \n",
    "    C = 1 / l\n",
    "    \n",
    "    score = dict(model=0, model_1=0)\n",
    "    loss = dict(model=0, model_1=0)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
    "\n",
    "        model = LogisticRegression(random_state=0, solver='saga', C=C, penalty='l2', n_jobs=4, max_iter=200)\n",
    "        model_1 = LogisticRegression(random_state=0, solver='saga', C=C, penalty='l1', n_jobs=4, max_iter=200)\n",
    "        \n",
    "        X_train = np.concatenate([radiants.iloc[train_idx].copy(), dires.iloc[train_idx].copy()], axis=1)\n",
    "        X_test = np.concatenate([radiants.iloc[test_idx].copy(), dires.iloc[test_idx].copy()], axis=1)\n",
    "\n",
    "        y_train = df.radiant_win.iloc[train_idx].values\n",
    "        y_test = df.radiant_win.iloc[test_idx].values\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        model_1.fit(X_train, y_train)\n",
    "        \n",
    "        ypreds = model.predict_proba(X_test)[:, 1]\n",
    "        ypreds_1 = model_1.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        score['model'] += accuracy_score(y_test, ypreds > 0.5)\n",
    "        loss['model'] += log_loss(y_test, ypreds)\n",
    "        \n",
    "        score['model_1'] += accuracy_score(y_test, ypreds_1 > 0.5)\n",
    "        loss['model_1'] += log_loss(y_test, ypreds_1)\n",
    "        \n",
    "        \n",
    "    for k in ['model', 'model_1']:\n",
    "        \n",
    "        score[k] /= n_splits\n",
    "        scores[k].append(score[k])\n",
    "\n",
    "        loss[k] /= n_splits\n",
    "        losses[k].append(loss[k])\n",
    "    \n",
    "print('\\nDone.')\n",
    "          \n",
    "############################################################################\n",
    "\n",
    "# print('Avg threshold', avg_threshold / (n_splits * len(models_dict)), '\\n\\n')\n",
    "# print('Number of features:', len(feature_columns))\n",
    "# print('Features:', feature_columns, '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
